---
title: DS202W - ✏️ W10 Summative
author: <47083>
output: html
self-contained: true
jupyter: python3
engine: jupyter
python: /opt/anaconda3/bin/python
editor:
  render-on-save: true
  preview: true
---

# Summative 1

## Outline of summative 
1. Part 1 - Yaun Dynasty: Exploratory Data Analysis
2. Part 2 - Yuan Dynasty: Regression modeling
3. Part 3 - Bank of England: Classification models


### Part 1 - Yuan Dynasty
Abstract - The Yuan were a Mongol dynasty that ruled for only around a century (from 1260–1368), but they had a long-lasting influence on China’s culture, economy, and politics. They were also the first political regime in history that pegged paper money to precious metals and the first that deployed fiat money as the sole legal tender. 

#### Part 1 Outline
1. Imports
2. Explore data and derive insights
3. Answer the following questions:
  a. What are the years with the top 10 highest number of total wars? How many of those years (and which ones) overlap with the years with the top 10 nominal money issues?
  
  b. Similarly, what are the years with the top 10 highest number of disasters? How many of those years (and which ones) overlap with the years with the top 10 nominal money issues?
  
  c. Create a single plot that shows the evolution over time of CPI, total wars, disasters and nominal money issues (be mindful of variable scaling!). What does this plot tell you?


```{python}
#imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    roc_curve, 
    roc_auc_score, 
    precision_recall_curve, 
    average_precision_score,
    confusion_matrix, 
    classification_report
)
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
```

2.  Explore data and derive insights

```{python}
#Data loaded to yuan
yuan = pd.read_stata("Data/yuan_inflation_data.dta")

#data exploration
print("Dataset information:")
print(yuan.info())

#first 10 rows with a parapgraph break to get a clear viewing of the data
print("\nFirst few rows:")
print(yuan.head(10))


#.descibe to get summary statistics 
print("\nBasic statistics:")
print(yuan.describe())
```

---

3. Answer the following questions:

### 3a. What are the years with the top 10 highest number of total wars? How many of those years (and which ones) overlap with the years with the top 10 nominal money issues?

```{python}
#create a new df with the year and totalwar 
top_wars = yuan[['year', 'totalwar']]
#sort the dataframe by the totalwar column in descending order 
top_wars = top_wars.sort_values(by='totalwar', ascending=False).head(10)


#same thing for nominal money issues
top10_nominal_money = yuan[['year', 'nominal']]
top10_nominal_money = top10_nominal_money.sort_values(by='nominal', ascending=False).head(10)

#I wont print as I will put the output in markdown format for analysis
```

#### I am transforming each column that has the years from its original form into a Python set, in order to find the intersection of the two sets.

table |
|------|---------|
| 1352 | 40 |
| 1275 | 30 |
| 1355 | 26 |
| 1328 | 17 |
| 1325 | 12 |
| 1353 | 11 |
| 1327 | 11 |
| 1331 | 11 |
| 1354 | 10 |
| 1323 | 10 |
---

The top 10 years with the most nominal money issues are 
Top 10 years with highest nominal money issues:

| year | nominal |
|------|---------|
| 1355 | 49500000 |
| 1310 | 36259200 |
| 1354 | 34500000 |
| 1353 | 19500000 |
| 1352 | 19500000 |
| 1312 | 11211680 |
| 1311 | 10900000 |
| 1313 | 10200000 |
| 1314 | 10100000 |
| 1302 | 10000000 |

---

```{python}
overlapping_years = set(top_wars['year']).intersection(set(top10_nominal_money['year']))
print(overlapping_years)

#set creates a data structure  of unique items
# intersection finds the common items between the two sets
```


#### The years that overlap are **1352, 1353, 1354, 1355**  



### 3b.  Similarly, what are the years with the top 10 highest number of disasters? How many of those years (and which ones) overlap with the years with the top 10 nominal money issues?

```{python}
#we essentially need to repeat the same process as above, but with the disaster column
top10_disasters= yuan[['year', 'disaster']]
top10_disasters = top10_disasters.sort_values(by='disaster', ascending=False).head(10)


print("Top 10 years with highest number of disasters:")
print(top10_disasters)
```

```{python}
#utilize set function again to find the intersection of the two sets (disasters and nominal money issues)
years_overlap = set(top10_disasters['year']).intersection(set(top10_nominal_money['year']))
print("\nOverlapping years between disasters and nominal money issues:")
print(years_overlap)
```

3b. Because nothing printed **no years overlap!**




### 3c. Create a single plot that shows the evolution over time of CPI, total wars, disasters and nominal money issues (be mindful of variable scaling!). What does this plot tell you?

```{python}
# we need to normalize the data to make it comparable

#use the formula min  max scaling - it transforms each value by subtracting the minimum value and dividing by the range (maximum minus minimum), 
yuan['CPI_normalized'] = (yuan['cpi'] - yuan['cpi'].min()) / (yuan['cpi'].max() - yuan['cpi'].min())

yuan['wars_normalized'] = (yuan['totalwar'] - yuan['totalwar'].min()) / (yuan['totalwar'].max() - yuan['totalwar'].min())


yuan['disasters_normalized'] = (yuan['disaster'] - yuan['disaster'].min()) / (yuan['disaster'].max() - yuan['disaster'].min())


yuan['nominal_normalized'] = (yuan['nominal'] - yuan['nominal'].min()) / (yuan['nominal'].max() - yuan['nominal'].min())
```

---

#### Note before plotting: I believe that the boxplot is the best way to visualize the distribution of the data as it shows the median, quartiles, and outliers, which help for comparison of multiple variables. However I will use more than 1 plot in order to make sure the data is being visualized thoroughly.

```{python}

# We are using the standard scaler to normalize the data this time.
scaler = StandardScaler()

# New df with scaled data
scaled_data = scaler.fit_transform(yuan[['cpi', 'totalwar', 'disaster', 'nominal']])


yuan_scaled = pd.DataFrame(scaled_data, columns=['CPI_normalized', 'wars_normalized', 'disasters_normalized', 'nominal_normalized'])

plt.figure(figsize=(12, 8))
plt.plot(yuan['year'], yuan_scaled['CPI_normalized'], label='CPI', color='blue', linewidth=2)
plt.plot(yuan['year'], yuan_scaled['wars_normalized'], label='Total Wars', color='red', linewidth=2)
plt.plot(yuan['year'], yuan_scaled['disasters_normalized'], label='Disasters', color='green', linewidth=2)
plt.plot(yuan['year'], yuan_scaled['nominal_normalized'], label='Nominal Money Issues', color='orange', linewidth=2)
plt.title('CPI, Total Wars, Disasters, and Nominal Money Issues Over Time of the Yuan Dynasty')
plt.xlabel('Year')
plt.ylabel('Z-score')
plt.legend(loc='upper left')
plt.grid(True, which='both', linestyle='--', alpha=0.7)
plt.tight_layout()


save_path = "yuan_dynasty_variables.png"
plt.savefig(save_path, dpi=300, bbox_inches='tight')
plt.show()

# Print the absolute path to the saved image
abs_path = os.path.abspath(save_path)
print(f"Image saved at: {abs_path}")
```

```{python}
#first plot boxplot, this uses the normalized data. 
plt.figure(figsize=(10, 6))
boxplot_data = [yuan_scaled['CPI_normalized'], yuan_scaled['wars_normalized'], 
                yuan_scaled['disasters_normalized'], yuan_scaled['nominal_normalized']]
plt.boxplot(boxplot_data, labels=['CPI', 'Wars', 'Disasters', 'Nominal'])
plt.title('Distribution Comparison of CPI, Total Wars, Disasters, and Nominal Money Issues - Yuan Dynasty (1260-1355)')
plt.ylabel('Normalized Value')
plt.grid(axis='y', alpha=0.3)
```

## Analysis of the data
## We can break down the data into 2 parts: 
  1. Each variable individually.
  2. The data as a whole.


## Breakdown each of the variables that we analyzed.

### - CPI
The first plot shows the normalized data of all the variables over time. We can see that CPI is the most stable, however, after 1340 there was a spike, and ends with a z score of around 3.8. This makes sense as in the paper it is stated after 1340 population started to decline "population started to decline "Population figures during the mid-Yuan period (1290−333) remained unchanged but started to recover after 1330 and peaked in 1341.51 After that, the population started to decline." (Guan, Palma, and Wu 2024). 


#### - Total Wars
In terms of the other variables, we can see that the total wars, is very volatile with sharp spikes, paticularly around 1275, with a z score of 4.2, and ends with a score of 3.5 in 1355 after its peak a few years prior. This is makes sense as it suggests that the total wars were not as frequent as the CPI, but when they were frequent, they were devestating. 

#### - Nominal Money Issues
Although having a peak at the end, nominal money issues is by far the most stable, and the lowest of all the variables. It spikes in 1310, which can be explained by the abondonment of the The third paper money, the zhidachao, the the third paper money issued by the issued by the third Yuan emperor, KülügYuan, in 1310. It had a It had a significantly higher value than previous currencies:
  1 guan of zhidachao was equivalent to 5 guan of zhiyuanchao
  1 guan of zhidachao was equivalent to 25 guan of zhongtongcha 
However, the zhidachoa was only in cirualtion for 1 year, as soon after,  Külüg died suddenly in 1311, his successor Ayurbarwada Khan abandoned the zhidachao and restored the previous two paper currencies (zhongtongchao and zhiyuanchao). The issuance of zhidachao represented a massive spike in the nominal money supply, with annual issuance reaching 36 million ding in 1310, compared to just 5 million ding when zhiyuanchao was being issued. This can explain why the spike in 1310 is so high. 

#### - Disasters
The disasters are arguably the most volatile of all the variables based on the z-score plot. There a few peaks as seen in the 1290s and the 1330s. There was years of tranquility prior to the 1290s, as in the paper, it states, "After the initial years of tranquillity, from 1285, the empire began to suffer from various natural disasters" (Frequency of natural disasters. Source: Chen et al., A chronicle of natural and man-made disasters in China, pp. 1068−220.). 

### Data as a whole
As mentioned, each of these variables individually have differing scales, ranges, and historical context that make them difficult to compare. However, we can still derive that CPI was the most affected by the other variables, as CPI when measuring inflation, there is no such thing as one factor/variable that affects it, it is a cumulation of many factors. These factors we measured, total wars, disasters, and nominal money issues, all have an effect on the CPI. With that in mind, the CPI was still relatively stable, and with a simple plot, it is hard to tell the true effect the other variables, if there was a significant effect. If there were to be more extensive analysis, we could use a correlation matrix to see the relationship between the variables, or another model 
 📊 😅...
 
---


# Part 2: Create regression models (45 marks)


## Overview of steps for part 2
1. Create a baseline linear regression model:
  a. Create the training and test sets:
  b. linear regression model that predicts the CPI
  c. Residuals plot
  d. Model evaluation/performance metrics

2.  Come up with my own feature selection or feature engineering or model selection strategy and try to get a better model performance than you had before. 
  a. Model selection choice and justification
  b. Code
  c. Explaination of choices
  d. Model performance/evaluation
  e. Comparison to baseline model


### 1a. 

#### Training model
Our focus is to try to predict the CPI after 1327. Our first model is based on the 4 factors we have been using throughout the assignment:

The machine learning algorithm is trained on a dataset, as the testing data is used to determine the performance of the trained model, whereas training data is used to train the machine learning model. (geeks for geeks). That is why we split the data. 

```{python}

yuan = pd.read_stata("Data/yuan_inflation_data.dta")
yuan_train = yuan[yuan['year'] < 1327].copy()
yuan_test = yuan[yuan['year'] >= 1327].copy()


features = ['totalwar', 'disaster', 'nominal']
target = 'cpi'


X_train = yuan_train[features]
y_train = yuan_train[target]

#Add constant term for intercept
X_train_const = sm.add_constant(X_train)

#Train the OLS model on training data (pre-1327)
ols_model = sm.OLS(y_train, X_train_const)
results = ols_model.fit()


print("OLS Model for Training Data:")
print(results.summary())


fitted_values = results.fittedvalues
residuals = y_train - fitted_values
train_r2 = results.rsquared
train_rmse = np.sqrt(mean_squared_error(y_train, fitted_values))

print("\nPerformance on Training Data:")
print("R-squared:", np.round(train_r2, 2))
print("Root Mean Squared Error (RMSE):", np.round(train_rmse, 2))

plt.figure(figsize=(10, 6))
plt.scatter(fitted_values, residuals, alpha=0.7, color='blue')
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('CPI Values')
plt.ylabel('Residuals')
plt.title('Residuals Plot for Training Data')
plt.grid(True)
plt.show()

```


##### Explaination

The model is specifically looking at how three variables affect inflation (CPI) during the pre-1327 period:
  a. totalwar - The amount of warfare/conflict
  b. disaster - Natural disasters
  c. nominal - Likely a measure of nominal currency values
Important Statistics - The r-squared value of 0.481 show us that the 48.1% of the variation in CPI during this period. This makes sense; before 1327 the silver standard was still in place. This means that the Yuan dynasty was still using silver as its currency, and silver was still a major factor in the economy. 

#### 1d. Model evaluation/performance metrics

##### Model Comparison: Training data:
 Overall, the model is not horrible. Considering the r-sqaured values is 0.48, the model is moderate. It accounts for almost 50% of the variation for the CPI... in all fairness for 3 variables, it is not too bad. This makes sense however, as the CPI is not just defined by the variables we included (total wars, disasters, and nominal money issues), but also by other factors that are not included in the model. The f-statistic is less than 0.00001, which is good, thus meaning the information we included is statistically significant. The p-value for disaster and nomial p-value are 0.0000, which is significant. However, the p-value for total wars is 0.259, which is not significant. This makes some sense as the wars seen in the last plot from part 1, show that the wars were not as frequent as the other inherirent issues such as nominal money issues and disasters which followed a similar volatility. Additionally, if the wars were less frequent, they may not have had as much of an immediate or lasting impact on inflation in the same way that disasters or nominal issues did. CPI however is a special variable. This is evident when exploring the last plot from part 1, where the disasters are realievly stable **until 1340**, whre there is a massive spike.  The CPI spike in 1340 might be tied to an event not captured well in the model (such as a specific, one-time shock), that could explain why totalwar isnt significant and  other variables might be significant. Lets test my hypothesis out to create my own model. 


#### pre test analysis final note -

The training data might be regarded as not being important but it helps contextualize the results we will get from the testing data.  It essentially helps forecast/foreshadows the results. Consiering the results of predicting cpi we have are already quite low, the test results will intuitvely be worse. Lets see if this is true. 

#### Testing Model
```{python}


X_test = yuan_test[features]
y_test = yuan_test[target]
X_test_const = sm.add_constant(X_test)

#We use the pre-1327 trained model to predict post-1327 data
y_test_pred = results.predict(X_test_const)  
test_r2 = r2_score(y_test, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("Performance of Training Model on Test Data:")
print("Test R-squared:", np.round(test_r2, 2))
print("Test Root Mean Squared Error (RMSE):", np.round(test_rmse, 2))
```



##### Model Comparison: Test data:
Performance on Test Data (using training model):
  Test R-squared: -1.48
  Test Root Mean Squared Error (RMSE): 42.35

  Why the hell is the r-squared value negative?

This shows us that the test model is way way way less accurate than the training model, as the r-squared value is lower, actually negative, and the RSME is much higher. 
Analysis: The model couldn't predict post-1327 inflation because the rules of the economy had changed.
The poor cross-period prediction performance actually validates the historical narrative about regime changes.  Im grasping for meaning. Is it really because the model is bad? Or is it because the model is trying to predict a different economy? 

We can decipher why the model performed poorly in the test data by looking at the historical context. 
In the paper, it breaks down the Yuan dynasty into 3 periods:
  1. Full silver convertibility (1260-1275)
  2. Nominal silver convertibility (1276-1309)
  3. Fiat standard (1310-1368)

 Yuan government began to print the third paper money (zhidachao), silver no longer played a reserve role, and the monetary system officially changed to a de jure fiat money system. The paper also talks about how that "under the fiat standard, total warfare and civil war were much more likely to increase nominal money issues." 
 Their statistical analysis shows that warfare had a much stronger effect on monetary expansion during the fiat period than during earlier periods.The poor performance of our models on post-1327 data aligns with the paper's general finding that the fiat standard period had different economic dynamics. This will be important later on.



## 2: My own model : 

#### Features: 
I want to add more features to the model. This will help predict CPI better, as more data = better results. I want to add variables but some variables are similar in nature, such as rebellion and total war. There is evidence for this working, as in Table 6 (page 1241), the paper shows statistical results indicating that the impact of warfare on money issues is only significant during the fiat standard (1310-1368), and not as significant during the silver standard periods (1260-1275 and 1276-1309). I know however this will not fully anwer my question, but will be helpful for solving the mystery. 

#### My statistical question: What would happen if I enhanced the model by adding more variables that took into account all forms of unrest?


```{python}
#feature engineering - combine all of warfare into 1 variable: total_conflict

yuan['total_conflict'] = yuan['external'] + yuan['unification'] + yuan['totalwar']

#combine all of unrest into 1 variable: total_unrest
yuan['total_unrest'] = yuan['total_conflict'] + yuan['rebellion']
yuan_train = yuan[yuan['year'] < 1327].copy()
yuan_test = yuan[yuan['year'] >= 1327].copy()

features = ['total_unrest', 'disaster', 'nominal', 'pop']
target = 'cpi'
X_train = yuan_train[features]
y_train = yuan_train[target]
#Add a constant as the intercept
X_train_const = sm.add_constant(X_train)


ols_model = sm.OLS(y_train, X_train_const)
results = ols_model.fit()
print("OLS Model for Training Data:")
print(results.summary())


fitted_values = results.fittedvalues
residuals = y_train - fitted_values
r2 = results.rsquared
rmse = np.sqrt(mean_squared_error(y_train, fitted_values))
print("\nPerformance on Training Data:")
print("R-squared:", np.round(r2, 2))
print("Root Mean Squared Error (RMSE):", np.round(rmse, 2))

plt.figure(figsize=(10, 6))
plt.scatter(fitted_values, residuals, alpha=0.7, color='blue')
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('CPI Values')
plt.ylabel('Residuals')
plt.title('Residuals Plot for Training Data')
plt.grid(True)
plt.show()
```

##### Analysis of new model with enhanced features:
Wow! The r-squared value for the training data is 0.931, which is way higher than the previous model. 
This shows that the new model is more accurate. The RMSE is also lower, which shows that the new model is more accurate. 
Howvever, the testing data is the important part. 



```{python}

X_test = yuan_test[features]
y_test = yuan_test[target]

#
X_test_const = sm.add_constant(X_test)


ols_test_model = sm.OLS(y_test, X_test_const)
results_test = ols_test_model.fit()


print("\nOLS Model for Test Data:")
print(results_test.summary())


# Use the pre-1327 trained model (results from training) to predict post-1327 data
y_test_pred = results.predict(X_test_const)


test_residuals = y_test - y_test_pred

#Calculate performance metrics on the test data
test_r2 = r2_score(y_test, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("\nPerformance on Test Data:")
print("Test R-squared:", np.round(test_r2, 2))
print("Test Root Mean Squared Error (RMSE):", np.round(test_rmse, 2))


plt.figure(figsize=(10, 6))
plt.scatter(y_test_pred, test_residuals, alpha=0.7, color='green')
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Fitted CPI Values (Test Data)')
plt.ylabel('Residuals')
plt.title('Residuals Plot for Test Data')
plt.grid(True)
plt.show()

```


### Model Analysis:
I got a negative r-squared value again. Why? Maybe random forest will work better. 

```{python}
from sklearn.ensemble import RandomForestRegressor
# Load the data
yuan = pd.read_stata("Data/yuan_inflation_data.dta")

# Feature engineering
yuan['total_conflict'] = yuan['external'] + yuan['unification'] + yuan['totalwar']
yuan['total_unrest'] = yuan['total_conflict'] + yuan['rebellion']  # Fixed syntax

features = ['total_unrest', 'disaster', 'nominal', 'pop']
target = 'cpi'

# Train-test split
train = yuan[yuan['year'] < 1327].copy()
test = yuan[yuan['year'] >= 1327].copy()

X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]

# Train model (no scaling needed)
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=10,
    min_samples_leaf=2,
    random_state=42
)
rf_model.fit(X_train, y_train)  # Use raw features

# Predictions
y_train_pred = rf_model.predict(X_train)
y_test_pred = rf_model.predict(X_test)

# Performance metrics (original scale only)
train_r2 = r2_score(y_train, y_train_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("\nPerformance on Training Data:")
print(f"R-squared: {train_r2:.2f}")
print(f"RMSE: {train_rmse:.2f}")

print("\nPerformance on Test Data:")
print(f"R-squared: {test_r2:.2f}")
print(f"RMSE: {test_rmse:.2f}")

# Feature importance
importances = rf_model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest Model')
plt.show()
```


#### The code explained:
This model uses a Time-based split:
  Uses year < 1327 for training and >= 1327 for testing. 
  This makes the model realistic for forecasting.

#### Results: 
Training Data:
- The R^2 is 1.00, it should be high as it is basing this on the training data. This is the problem (looking back at it I hadnt noticed yet, but I am not going to take it out because it helps me build a story out of the data).

Test Data:
- The r^2 is -1.16, the model is horrible at predicting the test data. Lets try to figure out why. 

- The Average R^2 for time series = -35.59, this is very concerning.
The model performs terribly on all time splits, which suggests it generalizes poorly even across different historical time periods.
The negative R^2 values indicate that the model does worse than simply predicting the mean value.

This sucks, but it clearly means that random forest is not a good model **This model is not only worse it is negative.**



### Why the negative value makes sense:
As I mentioned before, the Yuan dynasty underwent a regime change in 1327. This means that the rules of the economy changed. The silver standard was no longer in place, and the Yuan dynasty was now using a fiat currency. This means that the model is now trying to predict a different economy, which is why it is performing poorly. Also in the original graph, (will include another photo of it now) the CPI shoots up after 1327, which is why the model is performing poorly. The whole economy changed and the cpi skyrocketed, why is why the model is performing poorly. 

![Yuan Dynasty Variables](images/yuan_dynasty_variables.png)


#### Lasso Model: 
Since Random Forest is struggling, I will try to use a lasso model (didnt know at the time but this was not the core problem) - this essentially has two key features that make it a good fit for this data:

  - Regularization: Lasso adds a penalty term to the standard linear regression cost function. This penalty is proportional to the absolute value of the coefficients (L1 regularization).
  
  - Feature selection: Unlike Ridge regression (which uses L2 regularization), Lasso can reduce some coefficients to exactly zero. This effectively removes those features from the model, resulting in a sparse model.


#### Code:
### For the training and testing model:

```{python}



yuan = pd.read_stata("Data/yuan_inflation_data.dta")


yuan['total_conflict'] = yuan['external'] + yuan['unification'] + yuan['totalwar']
yuan['total_unrest'] = yuan['total_conflict'] + yuan['rebellion']


features = ['total_unrest', 'disaster', 'nominal', 'pop']
target = 'cpi'


train = yuan[yuan['year'] < 1327].copy()
test = yuan[yuan['year'] >= 1327].copy()

X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


param_grid = {'alpha': np.logspace(-4, 1, 50)}  
tscv = TimeSeriesSplit(n_splits=5)

lasso_cv = GridSearchCV(
    Lasso(max_iter=10000, random_state=42), 
    param_grid, 
    cv=tscv,
    scoring='neg_mean_squared_error'
)

lasso_cv.fit(X_train_scaled, y_train)
best_alpha = lasso_cv.best_params_['alpha']
print(f"Best alpha value: {best_alpha}")


lasso_model = Lasso(alpha=best_alpha, max_iter=10000, random_state=42)
lasso_model.fit(X_train_scaled, y_train)


y_train_pred = lasso_model.predict(X_train_scaled)
y_test_pred = lasso_model.predict(X_test_scaled)


train_r2 = r2_score(y_train, y_train_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

test_r2 = r2_score(y_test, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("\nPerformance on Training Data:")
print(f"R-squared: {train_r2:.2f}")
print(f"RMSE: {train_rmse:.2f}")

print("\nPerformance on Test Data:")
print(f"R-squared: {test_r2:.2f}")
print(f"RMSE: {test_rmse:.2f}")


coef = pd.Series(lasso_model.coef_, index=features)
importance = pd.DataFrame({'Feature': features, 'Coefficient': coef.values})
importance = importance.sort_values(by='Coefficient', key=abs, ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance['Feature'], importance['Coefficient'], color='skyblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Coefficients in Lasso Model')
plt.axvline(x=0, color='gray', linestyle='-', alpha=0.7)
plt.grid(True, axis='x', alpha=0.3)
plt.show()


plt.figure(figsize=(12, 6))
plt.plot(yuan['year'], yuan[target], 'b-', label='Actual')
plt.plot(train['year'], y_train_pred, 'r--', label='Predicted (Train)')
plt.plot(test['year'], y_test_pred, 'g--', label='Predicted (Test)')
plt.axvline(x=1327, color='k', linestyle='--', label='Train/Test Split')
plt.xlabel('Year')
plt.ylabel('CPI')
plt.title('Actual vs. Predicted CPI Values Over Time (Lasso Model)')
plt.legend()
plt.grid(True)
plt.show()


non_zero = coef[coef != 0].sort_values(key=abs, ascending=False)
print("\nNon-zero coefficients (features selected by Lasso):")
for feature, coefficient in non_zero.items():
    print(f"{feature}: {coefficient:.6f}")

```

### Results of lasso model:
Still negative r-squared value. Why?


#### Plotting the actual vs predicted values:
```{python}
#creates a full index that spans the full dataset length
full_index = np.arange(len(y_train) + len(y_test))


actual_values = np.concatenate([y_train.values, y_test.values])

#concatenates the  predicted values (train + test)
predicted_values = np.concatenate([y_train_pred, y_test_pred])


plt.figure(figsize=(12, 6))
plt.plot(full_index, actual_values, label='Actual Values', color='blue', alpha=0.7)
plt.plot(full_index, predicted_values, label='Predicted Values', color='red', linestyle='--')


plt.axvline(x=len(y_train), color='black', linestyle='dashed', label='Train/Test Split')

plt.xlabel('Time')
plt.ylabel('CPI')
plt.title('Actual vs. Predicted CPI Values Over Time')
plt.legend()
plt.grid(True)
plt.show()

```


### Analysis of the new model
After the split (vertical black dashed line), the predicted values flatten out, while the actual values continue to increase sharply. The model does decently well given the lack of data and variables that go into account when predicting a variable of complexity such as CpI. 

 

**This doesn't make sense. There is still an underlying fundamental issue with not the model, but my model (the way ive structured it) in general. The reason the r-sqaured wsa high before was because I realized that my model was just improperly creating a new model directly on post-1327 data.**

### Model analyis - more conculsive result -
It took me a good hour or so to actually interpret why I was getting a negative r-squared value. Before, I was getting a postive r-squared (0.756) value. I realzed this was because I was using the pre-1327 model to predict the post-1327 data. This is not only a bad practice, but it is also not a good idea to use the training data to predict the test data. 
This is interesting and heartbreaking. The model is not good at predicting the cpi post 1327. My hypothesis is still attempting to look at the positive side/theoretical side of the data. The models poor performance can be related to the economic factors and inflation fundamentally changed after the Yuan Dynasty's monetary reform. The inability of pre-1327 patterns to predict post-1327 inflation isn't a modeling failure – it's evidence supporting the historian's view that the transition to a fiat standard transformed economic dynamics. 


### One last dance: New model without simply just trying new techniques
#### New Model Choice Explaintation

My new model isn't neccesarlily going to take into account a type of model to cater towards the data (i wont use a random forest model for example because it isnt ideal for the situation). Instead I will use a model that will help predict the cpi better knowing the different regimes. Here are the changes I will utilize: 

1. Modeling the regime change explicitly instead of trying to force one model to work across both periods. 
   - A fiat_regime indicator variable (1 for post-1327, 0 for pre-1327)
   - A post_1340 indicator to capture the sharp acceleration in inflation

2. Adding interaction terms between each predictor and the regime indicator:
   - total_unrest_x_fiat
   - disaster_x_fiat
   - nominal_x_fiat
   - pop_x_fiat
3. Using the entire dataset instead of splitting it into training/test periods. This allows the model to learn the specific effects of each variable within each regime. 
4. Allowing coefficients to vary by regime rather than assuming consistent relationships throughout the dynasty.

**The most important conceptual change was recognizing that the negative R² values in your previous models weren't a failure but rather evidence of a fundamental economic shift. Instead of seeing this as a modeling problem to overcome, I incorporated it as a key feature of the data.**



```{python}
yuan['total_conflict'] = yuan['external'] + yuan['unification'] + yuan['totalwar']
yuan['total_unrest'] = yuan['total_conflict'] + yuan['rebellion']

yuan['fiat_regime'] = (yuan['year'] >= 1327).astype(int)  # 1 for fiat period, 0 for silver period
yuan['post_1340'] = (yuan['year'] >= 1340).astype(int)    # 1 for the period with rapidly accelerating inflation

#Create interaction terms
features = ['total_unrest', 'disaster', 'nominal', 'pop']
for feature in features:
    yuan[f'{feature}_x_fiat'] = yuan[feature] * yuan['fiat_regime']

model_features = features + [f'{feature}_x_fiat' for feature in features] + ['fiat_regime', 'post_1340']


X = yuan[model_features]
y = yuan['cpi']


X_const = sm.add_constant(X)


full_model = sm.OLS(y, X_const)
results = full_model.fit()


print("Model with Regime Interactions:")
print(results.summary())


predictions = results.predict(X_const)
r2 = r2_score(y, predictions)
rmse = np.sqrt(mean_squared_error(y, predictions))

print(f"\nOverall Model Performance:")
print(f"R-squared: {r2:.2f}")
print(f"RMSE: {rmse:.2f}")

#Evaluate performance separately for each regime
silver_mask = yuan['fiat_regime'] == 0
fiat_mask = yuan['fiat_regime'] == 1

silver_r2 = r2_score(y[silver_mask], predictions[silver_mask])
silver_rmse = np.sqrt(mean_squared_error(y[silver_mask], predictions[silver_mask]))

fiat_r2 = r2_score(y[fiat_mask], predictions[fiat_mask])
fiat_rmse = np.sqrt(mean_squared_error(y[fiat_mask], predictions[fiat_mask]))

print(f"\nSilver Regime (pre-1327) Performance:")
print(f"R-squared: {silver_r2:.2f}")
print(f"RMSE: {silver_rmse:.2f}")

print(f"\nFiat Regime (post-1327) Performance:")
print(f"R-squared: {fiat_r2:.2f}")
print(f"RMSE: {fiat_rmse:.2f}")

# Visualize actual vs. predicted
plt.figure(figsize=(12, 6))
plt.plot(yuan['year'], y, 'b-', label='Actual CPI')
plt.plot(yuan['year'], predictions, 'r--', label='Predicted CPI')
plt.axvline(x=1327, color='k', linestyle='--', label='Regime Change (1327)')
plt.axvline(x=1340, color='g', linestyle=':', label='Inflation Acceleration (1340)')
plt.xlabel('Year')
plt.ylabel('CPI')
plt.title('Actual vs. Predicted CPI with Regime Change Model')
plt.legend()
plt.grid(True)
plt.show()

# Analyse coefficient changes between regimes
print("\nEffect of predictors in Silver Regime (pre-1327):")
for feature in features:
    print(f"{feature}: {results.params[feature]:.6f}")

print("\nEffect of predictors in Fiat Regime (post-1327):")
for feature in features:
    print(f"{feature}: {results.params[feature] + results.params[f'{feature}_x_fiat']:.6f}")

```


### Results -

Holy crap. The new r-squared value is 0.85!!!! This is a huge improvement. also the actual versus predicted graph is much better. 


My new model is much better (R² = 0.94) because it recognizes that the economy worked differently before and after 1327 when the Yuan Dynasty changed from silver-backed money to paper money. The key changes i made were: 
```{python}
yuan['fiat_regime'] = (yuan['year'] >= 1327).astype(int)  # 1 for post-1327, 0 for pre-1327
yuan['post_1340'] = (yuan['year'] >= 1340).astype(int)    # 1 for acceleration period
```

These variables act like switches, telling the model when the economy changed its rules.

```{python}
for feature in features:
    yuan[f'{feature}_x_fiat'] = yuan[feature] * yuan['fiat_regime']
```

These allow each predictor (like warfare or money supply) to have different effects before and after the regime change. The "_x_fiat" means "multiplied by the fiat regime indicator." 

Also, instead of training on pre-1327 and testing on post-1327, the model used all the data while explicitly marking the different periods.

Finally, the interaction terms. This lets each variable in the model have one effect in the silver period and a different effect in the fiat period.

```{python}
model_features = features + [f'{feature}_x_fiat' for feature in features] + ['fiat_regime', 'post_1340']
```

**The biggest conceptual change was shifting from "can we predict the future using the past?" to "how did the economic rules change when the monetary system changed?"**

It was relieving to get my head wrapped around that. 





## Part 3 : Bank of England Interest Rates Data Set

### Overview:
1. Download the Bank of England interest rates dataset and load it into a dataframe called df. Carefully inspect the dataset and check that rate setting events occur every once in a while.
  a. Download the economic indicators dataset into another dataframe: your task is to assign to each row of df values of economic indicators from the last quarter i.e the average of each indicator for the last three months up to the date of the rate setting event. For e.g, if the rate setting event is on 06/05/1997, you will to average data for GDP for May 1997, April 1997 and March 1997 and do the same separately for the other indicators i.e exchange rates, 10-year gilt yield, unemployment rates, CPIH and CCI.
2. Create a baseline logistic regression model:

#### Part 3a: Bank of England
```{python}

# Load data set, explore it a little
df = pd.read_csv('Boe_interest_rates.csv')
df.info()
df.describe()
print(df.head(10))

#three columns, date, rate, and rate change

#when the rates increase, the rate change is positive, thus is 1, if it decreases, it is 0

#convert the date column to a datetime object
# Convert date column to datetime
df['Date'] = pd.to_datetime(df['Date'])
df.head(10)




#lets explore the distribution of time between rate changes

df['time_diff'] = df['Date'].diff()
print(df['time_diff'].head(10))



#diff - Calculates the difference of a DataFrame element compared with another element in the DataFrame (default is element in previous row).


#convert time_diff to a number
df['time_diff_days'] = df['time_diff'].dt.days





#graph it - use histogram it
plt.figure(figsize=(10, 6))
plt.hist(df['time_diff_days'], bins=20, edgecolor='black')
plt.xlabel('Time Between Rate Changes')
plt.ylabel('Frequency')
plt.title('Distribution of Time Between Rate Changes')
plt.show()



# The vast majority of events occur with a short time difference (around 30 days), which suggests that the Bank of England typically has monthly rate-setting meetings.

```

#### Now we can start with the economic indicators dataset
```{python}
#download the economic indicators dataset
economic_indicators = pd.read_csv('economic_indicators_interest_rate_setting.csv')
#assign each row of the df values of economic indicators from the last quarter i.e the average of each indicator for the last three months up to the date of the rate setting event. 
print(economic_indicators.head(10))

economic_indicators = economic_indicators.sort_values('Date')
print(economic_indicators.columns)

    
```

#### Now that we have explored the data, we can build a function to calculate the average of all of the economic indictors for the last quarter

##### Making sense of the problem:
Essentially, for each indicator we need to compute the average of the last 3 months up to the date of the rate setting event. In order to do this, we need to do some date manipulation prior: 
1. We need to rename/create a new dataframe for clarity
2. We should rename the columns to be more friendly/easier to work with for python
3. We need to Convert dates to datetime and Create monthly period columns
4. Then we can calculate prior 3 months for each event
5. We can actually use some sql/pandas functions to do this. We will be using explode and merge to manipulate and combine  the dataframes
6.  Now we can compute  averages for each indicator
 7. Finally, we can join the new dataframe back to the original df



```{python}

df_indicators = df
econ_df = economic_indicators
#new column ames
econ_df = econ_df.rename(columns={
    'Unemployment rate (aged 16 and over, seasonally adjusted): %': 'Unemployment_rate',
    '10-year-gilt-yield': '10_year_gilt_yield',
    'CPIH MONTHLY RATE 00: ALL ITEMS 2015=100': 'CPIH',
    'Gross Value Added - Monthly (Index 1dp) :CVM SA': 'GDP',
    'Monthly average Spot exchange rate, Sterling into US$              [a]             XUMAGBD': 'Exchange_rate_USD',
    'Monthly average Spot exchange rates, Sterling into Euro              [a]             XUMASER': 'Exchange_rate_EUR'
})

#convert dates to datetime
df['Date'] = pd.to_datetime(df['Date'])
econ_df['Date'] = pd.to_datetime(econ_df['Date'])

#  monthly period columns
df['event_month'] = df['Date'].dt.to_period('M')
econ_df['year_month'] = econ_df['Date'].dt.to_period('M')

#Calculate prior 3 months for each event
df['prior_months'] = df['event_month'].apply(lambda m: [m - i for i in range(1, 4)])

#Explode and merge
df_combined = df.explode('prior_months')
merged = df_combined.merge(
    econ_df,
    left_on='prior_months',
    right_on='year_month',
    how='left'
)
#sql techniques
print(df_combined.head(10)) 
indicators = ['GDP', 'Unemployment_rate', '10_year_gilt_yield', 'CPIH', 'CCI', 'Exchange_rate_USD']
averages = merged.groupby(merged.index)[indicators].mean()
averages.columns = [f'{col}_3m_avg' for col in averages.columns]


final_df = df.join(averages)
final_df.drop(columns=['event_month', 'prior_months'], inplace=True)

print("the final dataframe is:")
print(final_df.head(10))

print(final_df.columns)

```


#### Now we can start our model: The key thing to remember is we are going to try to predict. There are a few ways to go about this. We can try and use the data to predict the probability of a rate change, the direction of the rate change, or the size of the rate change. We can also go more in depth about the data and see how each indicator affects the rate change.

### Part 3b: Logistic Regression Model - Predicting the probability of a rate change

For this model, lets use the new dataframe we created and see if we can predict the probability of a rate change. We can do this by using the logistic regression model.


#### Outline of steps: 

1. All the neccesary imports
2. We already have the dataframe
3. We can prepare the features and target 
4. We can split the data into training and testing sets
5. Create and fit the model
6. Make predictions
7. Evaluate the model
8. print neccesary results 
  a. roc curve - This uses sensitivit, (the probability that the model predicts a positive outcome for an observation when the outcome is indeed positive.) and  specificity ( The probability that the model predicts a negative outcome for an observation when the outcome is indeed negative.) The ROC curve,  is a plot that displays the sensitivity and specificity of a logistic regression model. 
  b. Precision-Recall Curve - - This refers to the proportion of correct positive predictions (True Positives) out of all the positive predictions made by the model (True Positives + False Positives). It is a measure of the accuracy of the positive predictions.
  c. Model Evaluation Metrics (Classification Report) - This is a report that shows the precision, recall, f1-score, and support for each class.
  c. Confusion Matrix - A confusion matrix is a simple table that shows how well a classification model is performing by comparing its predictions to the actual results. It breaks down the predictions into four categories: correct predictions for both classes (true positives and true negatives) and incorrect predictions (false positives and false negatives). 
  d. feature_importance
  e. Probability Distribution
  f. cross validation  (on roc scores)

  
 I knew about most of these techinques, but got defintions and help from:
 **https://www.geeksforgeeks.org**


9. I will explain all of the results in detail
  
```{python}
from sklearn.linear_model import LogisticRegression #imports werent working
from sklearn.metrics import (
    roc_curve, 
    roc_auc_score, 
    precision_recall_curve, 
    average_precision_score,
    confusion_matrix, 
    classification_report
)

#features from the previous model
features = [
    'GDP_3m_avg', 
    'Unemployment_rate_3m_avg', 
    '10_year_gilt_yield_3m_avg', 
    'CPIH_3m_avg', 
    'CCI_3m_avg', 
    'Exchange_rate_USD_3m_avg'
]

#predicting rate change
X = final_df[features]
y = final_df['rate_change']


df_sorted = final_df.sort_values('Date')
split_index = int(len(df_sorted) * 0.7)


X_train = df_sorted.iloc[:split_index][features]
X_test = df_sorted.iloc[split_index:][features]
y_train = df_sorted.iloc[:split_index]['rate_change']
y_test = df_sorted.iloc[split_index:]['rate_change']

#scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

#predict probabilities
y_pred_proba = lr_model.predict_proba(X_test_scaled)
y_pred = lr_model.predict(X_test_scaled)

#roc curve
plt.figure(figsize=(10, 6))
fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])

plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.show()

#model evaluation metrics
print("Model Evaluation Metrics:")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

#confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['Decrease', 'Increase'])
plt.yticks(tick_marks, ['Decrease', 'Increase'])



plt.tight_layout()
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.savefig('confusion_matrix.png')
plt.show()

#Feature Importance
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': np.abs(lr_model.coef_[0])
})
feature_importance = feature_importance.sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.bar(feature_importance['feature'], feature_importance['importance'])
plt.title('Feature Importance in Predicting Rate Change')
plt.xlabel('Features')
plt.ylabel('Absolute Coefficient Value')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('feature_importance.png')
plt.close()

#probability distribution
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[:, 1], bins=20, edgecolor='black')
plt.title('Distribution of Predicted Probabilities')
plt.xlabel('Probability of Rate Increase')
plt.ylabel('Frequency')
plt.savefig('probability_distribution.png')
plt.show()

# Print out  10 probability predictions
print("\nSample Probability Predictions:")
for i, (actual, prob) in enumerate(zip(y_test, y_pred_proba[:, 1]), 1):
    if i <= 10:  # Print first 10 predictions
        print(f"Actual: {actual}, Predicted Probability of Rate Increase: {prob:.4f}")


prob_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted_Probability': y_pred_proba[:, 1]
})
prob_df.to_csv('rate_change_probabilities.csv', index=False)

# Detailed Feature Importance Print
print("\nFeature Importance:")
for _, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")
```



### Part 3b - Results: 
We have a lot of results to go through.
1. ROC Curve: 
- Limited discriminative power
- Suggests the model struggles to distinguish between classes effectively
- Indicates potential issues with feature selection or insufficient data

2. Precision-Recall Curve:
- Average precision score of 0.86 is actually  good
- Indicates the model has some ability to identify rate changes
- Contrasts with the poor ROC curve performance

3. Model Accuracy:
- 24% accuracy is low
- Reflects challenges in predicting rate changes
- Suggests complex, potentially non-linear relationships in the data

4. Confusion Matrix:
- Classic "accuracy paradox" scenario
- Model defaults to predicting the majority class
- Typical issue with imbalanced datasets

5. Feature Importance:
- GDP and 10-year gilt yield are most influential
- Provides insights for future model iterations
- Validates that some meaningful economic indicators are being considered

6. Probability Distribution:
- Low prediction probabilities (mostly < 0.5)
- Indicates model's lack of confidence in predictions
- Suggests need for:
  - More features
  - Different modeling approach
  - Addressing class imbalance

7. Cross-validation:
- ROC AUC below random chance (0.3375)
- Inconsistent performance
- Sample predictions confirm model's weakness

The possible next steps:
1. Address class imbalance
   - Techniques: 
     - Oversampling minority class
     - Undersampling majority class
     - SMOTE (Synthetic Minority Over-sampling Technique)

2. Feature Engineering
   - Create interaction terms
   - Add lagged features
   - Consider non-linear transformations

3. Try Different Algorithms
   - Random Forest
   - Gradient Boosting

4. Hyperparameter Tuning
   - Use grid search or random search
   - Focus on balancing parameters

5. Collect More Data
   - If possible, gather more historical data
   - Include additional economic indicators


#### Part 3c: New model:
I believe the best way to improve my model is with the following : Feature transformation:
1. Apply feature scaling: StandardScaler() or MinMaxScaler()
2. Create polynomial features: PolynomialFeatures(degree=2)
3. Add interaction terms between economic indicators
4. Use log transformations for skewed features Model specification changes: Change the target variable encoding (binary "change/no change" vs. multiclass "-1/0/1")
5. Add lagged features (include indicators from multiple time periods)
6. Include trend features (rate of change of indicators rather than just levels)
7. Combine logistic regression with other models like RandomForest

why? beause by applying techniques like scaling, polynomial features, log transformations, and incorporating lagged and trend features, I am  enabling the model to extract deeper insights from the data, moving beyond simple linear relationships and providing a more nuanced understanding of what drives interest rate changes.


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    roc_auc_score,
    precision_recall_curve,
    average_precision_score
)

# Feature Engineering Function
def create_advanced_features(df):
    # Create copy to avoid modifying original dataframe
    df_enhanced = df.copy()
    
    # 1. Lagged Features
    lag_features = [
        'GDP_3m_avg', 
        '10_year_gilt_yield_3m_avg', 
        'Unemployment_rate_3m_avg'
    ]
    
    for feature in lag_features:
        df_enhanced[f'{feature}_lag1'] = df_enhanced[feature].shift(1)
        df_enhanced[f'{feature}_lag2'] = df_enhanced[feature].shift(2)
    
    # 2. Trend Features (Rate of Change)
    for feature in lag_features:
        df_enhanced[f'{feature}_trend'] = df_enhanced[feature] - df_enhanced[f'{feature}_lag1']
    
# 3. Log Transformations (for potentially skewed features)
    log_features = [
        'GDP_3m_avg', 
        'Exchange_rate_USD_3m_avg'
    ]
    
    for feature in log_features:
        # Add small constant to avoid log(0)
        df_enhanced[f'{feature}_log'] = np.log(df_enhanced[feature] + 1)
    
    # Remove rows with NaN created by shifting
    df_enhanced.dropna(inplace=True)
    
    return df_enhanced

# Prepare Data
def prepare_model_data(df):
    # Enhanced feature creation
    df_enhanced = create_advanced_features(df)
    
    # Feature selection
    features = [
        'GDP_3m_avg', 
        '10_year_gilt_yield_3m_avg', 
        'Unemployment_rate_3m_avg',
        'Exchange_rate_USD_3m_avg',
        'CCI_3m_avg', 
        'CPIH_3m_avg',
        # Lagged features
        'GDP_3m_avg_lag1', 
        '10_year_gilt_yield_3m_avg_lag1', 
        'Unemployment_rate_3m_avg_lag1',
        # Trend features
        'GDP_3m_avg_trend',
        '10_year_gilt_yield_3m_avg_trend',
        # Log transformed features
        'GDP_3m_avg_log',
        'Exchange_rate_USD_3m_avg_log'
    ]
    
    X = df_enhanced[features]
    y = df_enhanced['rate_change']
    
    return X, y


def create_advanced_model():
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('scaler', StandardScaler()),
                ('poly', PolynomialFeatures(degree=2, include_bias=False))
            ]), slice(None))
        ])
    
    model_pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(
            n_estimators=100, 
            random_state=42, 
            class_weight='balanced'
        ))
    ])
    
    return model_pipeline

# Visualization Functions
def plot_roc_curve(y_true, y_pred_proba):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])
    roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])
    
    plt.figure(figsize=(10, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.savefig('advanced_roc_curve.png')
    plt.show()
    
    return roc_auc

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    
    classes = np.unique(y_true)
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)
    
 
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, str(cm[i, j]), 
                     horizontalalignment="center", 
                     color="white" if cm[i, j] > cm.max() / 2. else "black")
    
    plt.tight_layout()
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.savefig('advanced_confusion_matrix.png')
    plt.show()

# Main Execution
def run_advanced_model(final_df):

    X, y = prepare_model_data(final_df)
    
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Create and train model
    model = create_advanced_model()
    model.fit(X_train, y_train)
    
 
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)
    
    # Evaluation
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # ROC Curve
    roc_auc = plot_roc_curve(y_test, y_pred_proba)
    
    
    plot_confusion_matrix(y_test, y_pred)
    

    feature_importance = model.named_steps['classifier'].feature_importances_
    feature_names = model.named_steps['preprocessor'].get_feature_names_out()
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10 Feature Importances:")
    print(importance_df.head(10))
    
    return model, importance_df



def perform_cross_validation(final_df):
    X, y = prepare_model_data(final_df)
    model = create_advanced_model()
    

    cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
    
    print("\nCross-Validation Scores:")
    print(cv_scores)
    print(f"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")


run_advanced_model(final_df)
perform_cross_validation(final_df)

```

```{python}

X, y = prepare_model_data(final_df)


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


model = create_advanced_model()


model.fit(X_train, y_train)


y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)


print(classification_report(y_test, y_pred))

```

#### Part 3c - Results: 

__ROC Curve Analysis:__
AUC (Area Under Curve) is 0.61
Slightly better than random guessing (0.5)
Indicates modest predictive power
The curve shows some improvement over a random classifier, but not a strong discriminative model

__Confusion Matrix:__
True Negative (-1 class):
5 correctly predicted
2 misclassified
True Positive (1 class):
4 correctly predicted
3 misclassified




__Classification Report:__
Overall Accuracy: 0.57 (57%)
Precision:
-1 class: 0.56
1 class: 0.60


__Recall:__
-1 class: 0.71
1 class: 0.43

__Feature Importance Highlights:__
Top features:
10-year gilt yield
Interaction between GDP trend and gilt yield
Lagged gilt yield features
Interactions between unemployment, exchange rates, etc.

__Cross-Validation:__
scores vary widely: 0.398 to 0.798
Mean CV Score: 0.5367
High variance suggests model is unstable


Let's break down the results:

Firstly : There is an improvemnt in the results:
1. Confusion Matrix:
- More balanced predictions
- 5 correct predictions for -1 class
- 4 correct predictions for 1 class
- 2 misclassifications for -1 class
- 3 misclassifications for 1 class

2. Classification Report:
- Overall Accuracy: 0.57 (57%)
- Precision:
  - -1 class: 0.56
  - 1 class: 0.60
- Recall:
  - -1 class: 0.71
  - 1 class: 0.43
- More balanced performance across classes

3. ROC Curve:
- AUC remains 0.61
- Slightly better than random guessing
- Indicates modest predictive power

4. Feature Importance:
Top features remain consistent:
1. 10-year gilt yield
2. GDP trend and gilt yield interaction
3. Lagged gilt yield features

5. Cross-Validation:
- Scores range from 0.398 to 0.798
- Mean CV Score: 0.5367
- High variance suggests model instability


## Overview of Models

#### Baseline Logistic Regression Model

Accuracy: 24%
ROC AUC: Around 0.56
Major Issue: Severe class imbalance
Prediction Bias: Defaulting to majority class


#### Advanced Random Forest Model
Accuracy: 57%
ROC AUC: 0.61
Improved Prediction Balance
better Feature Engineering


There is a clear improvement in the results. The model is more balanced and has a higher AUC. The feature importance is also more reasonable.


## Sources
  **https://www.geeksforgeeks.org**
  **https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html**
  **https://www.nickmccullum.com/python-machine-learning/logistic-regression-python/**
  **https://claude.ai/chat**

##### I had some help from Claude and StackOverflow with debugging. However, all analysis is my own, in my own words, and my own work.